{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook we develop code for automatically binning for the unitary interpolation\n",
    "# goal is to reduce the equation: f(N_1,...N_n) = 2^(n-1) * prod_i=1^n N_i + N_1 * prod_i=n (N_i + 1) + N_n * prod_i=1^(n-1) N_i\n",
    "# where N_i are the bins in the directions i=1,...,n for the unitary interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "# generate latex table from arrays\n",
    "from Analysis_Code.useful import *\n",
    "import re\n",
    "\n",
    "def tracelessify_AB(A,B):\n",
    "    # Calculate the trace of A*B and A*A\n",
    "    trace_AB = np.trace(np.dot(A, B))\n",
    "    trace_AA = np.trace(np.dot(A, A))\n",
    "    # If the trace of A*A is not zero, calculate the multiple c and create the new matrix C\n",
    "    if np.abs(trace_AA) > 1e-10:  # We compare to a small number to avoid division by zero\n",
    "        c = trace_AB / trace_AA\n",
    "        C = B - c * A\n",
    "    else:\n",
    "        raise ValueError(\"The trace of A*A is zero, so we cannot create a new matrix C that fulfills Tr(AC) = 0.\")\n",
    "    return C\n",
    "\n",
    "# A function that orthogonalizes the Hamiltonians in H_s, so that their products are traceless.UI_directional_overlap_traces\n",
    "def orthogonalize_Hs(H_s):\n",
    "    # use tracelessify_AB to orthogonalize the Hamiltonians in H_s for every combination of i and j\n",
    "    n = H_s.shape[0]\n",
    "    H_s_ortho = H_s.copy()\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            H_s_ortho[j] = tracelessify_AB(H_s_ortho[i], H_s_ortho[j])\n",
    "    return H_s_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, eigh, qr\n",
    "from scipy.linalg import schur\n",
    "\n",
    "def Dag(U):\n",
    "    return np.transpose(np.conj(U))\n",
    "\n",
    "def vecxvec( E, wf, alpha): # Many times faster than multiply\n",
    "    s = E.size\n",
    "    for i in range(s):\n",
    "        wf[i] = np.exp(-1j * E[i] * alpha) * wf[i]\n",
    "    return wf\n",
    "# Construct matrix exponentials and products faster than in numpy or scipy\n",
    "def vm_exp_mul(E, V, dt = 1.0): # expm(diag(vector))*matrix  multiplication via exp(vector)*matrix\n",
    "    s = E.size\n",
    "    A = np.empty((s, s), np.complex128)\n",
    "    for i in range(s):\n",
    "        A[i,:] = np.exp(-1j * E[i] * dt) * Dag(V[:,i])\n",
    "    return A\n",
    "def expmH_from_Eig(E, V, dt = 1.0):\n",
    "    U = np.dot(V, vm_exp_mul(E, V, dt))\n",
    "    return U\n",
    "def expmH(H, dt = 1.0):\n",
    "    E, V = eigh(H)\n",
    "    return expmH_from_Eig( E, V, dt)\n",
    "\n",
    "##### Logarithms #########################################################\n",
    "def unitary_eig(A): # alternative to np.eig returning unitary matrices V\n",
    "    Emat, V = schur(A, output='complex')\n",
    "    return np.diag(Emat), V\n",
    "def vm_log_mul(E, V):\n",
    "    s = E.size\n",
    "    A = np.empty((s, s), np.complex128)\n",
    "    for i in range(s):\n",
    "        A[i,:] = np.log(E[i])*Dag(V[:,i])\n",
    "    return A\n",
    "def logmU(U):\n",
    "    E, V = unitary_eig(U)\n",
    "    return np.dot(V, vm_log_mul(E, V))\n",
    "\n",
    "def logmU_parts(U):\n",
    "    E, V = unitary_eig(U)\n",
    "    return -np.log(E).imag, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make hamiltonian from weights c and H_s --> H = H_s[0]+ sum_i c[i]*H_s[i+1]\n",
    "def H_from_c(H_s, c):\n",
    "    H = H_s[0].copy()\n",
    "    for i in range(len(c)):\n",
    "        H = H + c[i]*H_s[i+1]\n",
    "    return H\n",
    "\n",
    "# Get unitary from weights c and H_s\n",
    "def expm_H_s(H_s, c):\n",
    "    H = H_from_c(H_s, c)\n",
    "    E, V = eigh(H)\n",
    "    return expmH_from_Eig( E, V )\n",
    "\n",
    "# Construct the terms for corners of an interpolation (hyper-) cube\n",
    "def make_border_unitaries(H_s, c_mins, c_maxs):\n",
    "    if isinstance(c_mins, (list)):\n",
    "        c_mins = np.array(c_mins, dtype=np.float64)\n",
    "    if isinstance(c_maxs, (list)):\n",
    "        c_maxs = np.array(c_maxs, dtype=np.float64)\n",
    "    n = len(c_mins)\n",
    "    s = H_s.shape[-1]\n",
    "    U0 = expm_H_s(H_s, c_mins)\n",
    "    Ui = np.empty((n, s, s), dtype=np.complex128)\n",
    "    for i in range(n):\n",
    "        curr_c = c_mins.copy()\n",
    "        curr_c[i] = c_maxs[i]\n",
    "        Ui[i] = expm_H_s(H_s, curr_c)\n",
    "    n_ij = n * (n-1) // 2\n",
    "    Uij_2 = np.empty((n_ij, s, s), dtype=np.complex128)\n",
    "    Ui_2 = np.empty((n, s, s), dtype=np.complex128)\n",
    "    ind = 0\n",
    "    dc = (c_maxs - c_mins) \n",
    "    for i in range(n):\n",
    "        curr_c = c_mins.copy()\n",
    "        curr_c[i] += dc[i]/2\n",
    "        Ui_2[i] = expm_H_s(H_s, curr_c)\n",
    "        for j in range(i+1, n):\n",
    "            curr_c = c_mins.copy()\n",
    "            curr_c[i] += dc[i]/2\n",
    "            curr_c[j] += dc[j]/2\n",
    "            Uij_2[ind] = expm_H_s(H_s, curr_c)\n",
    "            ind += 1\n",
    "    curr_c = c_mins + dc/2\n",
    "    U_2 = expm_H_s(H_s, curr_c)\n",
    "    return U0, Ui, Ui_2, Uij_2, U_2\n",
    "\n",
    "# Get the unitary interpolation along \n",
    "# Construct interpolation terms from U0 to Ui\n",
    "def make_interpolation_terms(U0, Ui):\n",
    "    n = Ui.shape[0]\n",
    "    s = Ui.shape[-1]\n",
    "    Es = np.empty((n,s), dtype=np.float64)\n",
    "    Vs = np.empty((n,s,s), dtype=np.complex128)\n",
    "    for i in range(n):\n",
    "        U = np.dot(Ui[i], Dag(U0))\n",
    "        E, V = logmU_parts(U)\n",
    "        Es[i] = E\n",
    "        Vs[i] = V\n",
    "    return Es, Vs\n",
    "\n",
    "def interpolation_core_term(Vs, Es, index, alpha):\n",
    "    E = Es[index]\n",
    "    V = Vs[index]\n",
    "    return V @ np.diag(np.exp(-1j*E*alpha)) @ Dag(V)\n",
    "\n",
    "def nd_interpolation_core_term(Vs, Es, U0, alphas, indices):\n",
    "    U = interpolation_core_term(Vs, Es, indices[0], alphas[0])\n",
    "    for i in range(1, len(indices)):\n",
    "        U = np.dot(interpolation_core_term(Vs, Es, indices[i], alphas[i]), U)\n",
    "    return U @ U0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test directional accuracy of interpolation\n",
    "def UI_directional_overlap_traces(U0, Ui_2, Uij_2, U_2, Es, Vs):\n",
    "    # Test the quality of the interpolation for every combination (i,j) of directions at point x_i = 0.5 = x_j\n",
    "    # exception 1d: test only center\n",
    "    # Calculate and return the trace of the overlap operator M = U_exact^dagger @ U_interpolated (-d) , where d is the hilbert space dimension, which is subtracted except for all tr_M except the central element tr_M_2\n",
    "    n = Es.shape[0]\n",
    "    d = U0.shape[0] \n",
    "    if n == 1:\n",
    "        alphas = np.array([0.5])\n",
    "        indices = np.array([0])\n",
    "        U = nd_interpolation_core_term(Vs, Es, U0, alphas, indices)\n",
    "        tr_M = np.array([])\n",
    "        tr_M_single = np.array([np.trace(Dag(Ui_2[0]) @ U) - d])\n",
    "        tr_M_2 = tr_M_single[0]\n",
    "        indexes = np.empty((0,0), dtype=np.int64)\n",
    "    else:\n",
    "        n_ij = n * (n-1) // 2\n",
    "        tr_M = np.empty(n_ij, dtype=np.complex128)\n",
    "        tr_M_single = np.empty(n, dtype=np.complex128)\n",
    "        indexes = np.empty((n_ij, 2), dtype=np.int64)\n",
    "        ind = 0\n",
    "        alphas = np.array([0.5])\n",
    "        for i in range(n):\n",
    "            U = nd_interpolation_core_term(Vs, Es, U0, alphas, np.array([i]))\n",
    "            tr_M_single[i] = np.trace(Dag(Ui_2[i]) @ U) - d\n",
    "        alphas = np.array([0.5, 0.5])\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                indices = np.array([i,j])\n",
    "                U = nd_interpolation_core_term(Vs, Es, U0, alphas, indices)\n",
    "                tr_M[ind] = np.trace(Dag(Uij_2[ind]) @ U) - d - tr_M_single[i] - tr_M_single[j]\n",
    "                indexes[ind] = np.array([i,j], dtype=np.int64)\n",
    "                ind += 1\n",
    "    tr_M = tr_M + tr_M.conj()\n",
    "    tr_M = tr_M.real.astype(np.float64)\n",
    "    tr_M_single = tr_M_single + tr_M_single.conj()\n",
    "    tr_M_single = tr_M_single.real.astype(np.float64)\n",
    "    # compare central element\n",
    "    if n > 2:\n",
    "        alphas = np.array([0.5]*n)\n",
    "        indices = np.arange(n)\n",
    "        U = nd_interpolation_core_term(Vs, Es, U0, alphas, indices)\n",
    "        tr_M_2 = np.trace(Dag(U_2) @ U) - d\n",
    "        tr_M_2 = tr_M_2 + tr_M_2.conj()\n",
    "        tr_M_2 = tr_M_2.real.astype(np.float64)\n",
    "    elif n == 2:\n",
    "        tr_M_2 = tr_M[0]\n",
    "    else:\n",
    "        tr_M_2 = tr_M_single[0]\n",
    "    pref = -1/(d+1)\n",
    "    tr_M = pref * tr_M\n",
    "    tr_M_single = pref * tr_M_single\n",
    "    tr_M_2 = pref * tr_M_2\n",
    "    return tr_M_single, tr_M, tr_M_2, indexes\n",
    "\n",
    "def get_bidirectional_overlap_operators(H_s, c_mins=None, c_maxs=None, bins=None): # Calculate the infidelity parameters for a given interpolation\n",
    "    n = H_s.shape[0]-1\n",
    "    if c_mins is None:\n",
    "        c_mins = np.zeros(n)\n",
    "    if c_maxs is None:\n",
    "        c_maxs = np.ones(n)\n",
    "    if bins is None:\n",
    "        bins = np.ones(n, dtype=np.int64)\n",
    "    else:\n",
    "        bins = bins.astype(np.int64)\n",
    "    dc = (c_maxs - c_mins)\n",
    "    c_maxs = c_mins + dc / bins\n",
    "    U0, Ui, Ui_2, Uij_2, U_2_exact = make_border_unitaries(H_s, c_mins, c_maxs)\n",
    "    Es, Vs = make_interpolation_terms(U0, Ui)\n",
    "    tr_M_single, tr_M, tr_M_2, indexes = UI_directional_overlap_traces(U0, Ui_2, Uij_2, U_2_exact, Es, Vs)  # , tr_M_2\n",
    "    alphas = np.array([0.5]*n)\n",
    "    indices = np.arange(n)\n",
    "    U_2_approx = nd_interpolation_core_term(Vs, Es, U0, alphas, indices)\n",
    "    return tr_M_single, tr_M, tr_M_2, indexes, U_2_exact, U_2_approx\n",
    "\n",
    "def I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, param=None, bins=None):\n",
    "    if bins is None:\n",
    "        bins = np.ones(n, dtype=np.int64)\n",
    "    if param is None:\n",
    "        param = np.ones(n)\n",
    "    param = param / bins\n",
    "    res = 0.0\n",
    "    if len(indexes) > 0:\n",
    "        multipliers_2 = param[indexes[:,0]]**2*param[indexes[:,1]]**2\n",
    "        res += np.sum(multipliers_2 * tr_M)\n",
    "    multipliers_4 = param**4\n",
    "    res +=  np.sum(multipliers_4 * tr_M_single)\n",
    "    return res\n",
    "\n",
    "def I_from_tr_M(tr_M_single, tr_M, indexes, n, param=None):\n",
    "    return I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, param=param, bins=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|c|c|c|c|}\\hline\n",
      "n & s & rel. acc. & rel. acc. (Orthogonalized) \\\\\n",
      "\\hline\n",
      "$1$ & $4$ & $(6.59\\pm49.48)e-6$ & $(1.65\\pm8.79)e-6$ \\\\\n",
      "$1$ & $16$ & $(7.87\\pm9.54)e-9$ & $(1.06\\pm1.50)e-8$ \\\\\n",
      "$2$ & $4$ & $(3.31\\pm4.82)e-8$ & $(4.67\\pm8.05)e-8$ \\\\\n",
      "$2$ & $16$ & $(1.92\\pm0.95)e-8$ & $(1.64\\pm0.95)e-8$ \\\\\n",
      "$3$ & $4$ & $(2.71\\pm2.75)e-1$ & $(2.25\\pm2.24)e-1$ \\\\\n",
      "$3$ & $16$ & $(6.42\\pm4.72)e-2$ & $(7.24\\pm5.47)e-2$ \\\\\n",
      "$4$ & $4$ & $(3.37\\pm3.04)e-1$ & $(2.81\\pm2.55)e-1$ \\\\\n",
      "$4$ & $16$ & $(9.74\\pm7.69)e-2$ & $(8.32\\pm5.91)e-2$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy of the interpolation\n",
    "import Analysis_Code.discrete_quantum as dq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "printing = False\n",
    "s_vals, n_vals, orth_vals, unorth_vals = [], [], [], []\n",
    "rng = np.random.default_rng(100)\n",
    "for n in range(1,5):\n",
    "    for s in [4,16]:\n",
    "        reps = 100\n",
    "        rel_accs = np.empty(reps)\n",
    "        n_vals.append(n)\n",
    "        s_vals.append(s)\n",
    "        for ortho in range(2):\n",
    "            for r in range(reps):\n",
    "                H_s = dq.Random_parametric_Hamiltonian_gauss(n, s, sigmas=np.pi/2*np.array([1.0,0.025]), rng=rng)\n",
    "                if ortho:\n",
    "                    H_s = orthogonalize_Hs(H_s) # orthogonalized matrices are no better than unorthogonalized ones\n",
    "                tr_M_single, tr_M, tr_M_2, indexes, U_exact, U_approx = get_bidirectional_overlap_operators(H_s)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "\n",
    "                I_exact = dq.Av_Infidelity(U_exact, U_approx)\n",
    "                I_approx = I_from_tr_M(tr_M_single, tr_M, indexes, n)\n",
    "                # relative accuracy\n",
    "                rel_acc = np.abs(I_exact - I_approx) / I_exact\n",
    "                rel_accs[r] = rel_acc\n",
    "            if ortho:\n",
    "                orth_vals.append([np.mean(rel_accs), np.std(rel_accs)])\n",
    "                if printing:\n",
    "                    print('Orthogonalized:   n=' + str(n) + ' s='+str(s)+' Relative accuracy: ', np.mean(rel_accs), '+/-', np.std(rel_accs))\n",
    "            else:\n",
    "                unorth_vals.append([np.mean(rel_accs), np.std(rel_accs)])\n",
    "                if printing:\n",
    "                    print('Unorthogonalized: n=' + str(n) + ' s='+str(s)+' Relative accuracy: ', np.mean(rel_accs), '+/-', np.std(rel_accs))\n",
    "\n",
    "latex_str = latex_table(['n', 's', 'rel. acc.', 'rel. acc. (Orthogonalized)'], ['i', 'i', 'p2', 'p2'], [n_vals, s_vals, unorth_vals, orth_vals]) \n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original approximation:  2.747186064515758e-07 Exact:  2.984057987376332e-07\n",
      "Extrapolated approximation:  2.619920792117842e-13 Exact:  2.8699265186560297e-13\n",
      "Ratios:  9.5367431640625e-07  (extrapolated)  9.617529320130104e-07  (exact) \n"
     ]
    }
   ],
   "source": [
    "# estimate the infidelity for different binning and compare with actual accuracy\n",
    "import Analysis_Code.discrete_quantum as dq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 4\n",
    "s = 16  # Hilbert space dimension\n",
    "bins = 32\n",
    "param = np.ones(n)/bins   # scaled version\n",
    "rng = np.random.default_rng(100)\n",
    "\n",
    "rel_accs = np.empty(reps)\n",
    "H_s = dq.Random_parametric_Hamiltonian_gauss(n, s, sigmas=np.pi/2*np.array([1.0,0.025]), rng=rng)\n",
    "tr_M_single, tr_M, tr_M_2, indexes, U_exact, U_approx = get_bidirectional_overlap_operators(H_s)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "\n",
    "I_approx = I_from_tr_M(tr_M_single, tr_M, indexes, n)\n",
    "I_exact = dq.Av_Infidelity(U_exact, U_approx)\n",
    "print('Original approximation: ', I_approx, 'Exact: ', I_exact)\n",
    "\n",
    "I_approx2 = I_from_tr_M(tr_M_single, tr_M, indexes, n, param)\n",
    "_, _, _, _, U_exact2, U_approx2 = get_bidirectional_overlap_operators(H_s, bins= bins * np.ones(n, dtype=np.int64))  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms\n",
    "I_exact2 = dq.Av_Infidelity(U_exact2, U_approx2)\n",
    "print('Extrapolated approximation: ', I_approx2, 'Exact: ', I_exact2)\n",
    "\n",
    "print('Ratios: ',I_approx2/I_approx, ' (extrapolated) ', I_exact2/I_exact, ' (exact) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def cache_size_by_bins(bins):\n",
    "    # calculate the number of cache elements for a given binning\n",
    "    n = bins.shape[0] #len(bins)\n",
    "    if n > 1:\n",
    "        n_C = 2**(n-1) * np.prod(bins)\n",
    "        n_L = bins[0] * np.prod(bins[1:] + 1)\n",
    "        n_R = bins[-1] * np.prod(bins[:-1] + 1) \n",
    "        N_tot = n_C + n_L + n_R\n",
    "    else:\n",
    "        N_tot = bins[0]\n",
    "    return N_tot\n",
    "\n",
    "def cache_size_change_by_dir(bins, negative=False):\n",
    "    # check by how much the cache grows for every direction it could change in\n",
    "    n = bins.shape[0] #len(bins)\n",
    "    curr_size = cache_size_by_bins(bins)\n",
    "    differences = np.zeros(len(bins))\n",
    "    for i in range(n):\n",
    "        curr_bins = bins.copy()\n",
    "        if not negative:\n",
    "            curr_bins[i] += 1\n",
    "        else:\n",
    "            curr_bins[i] -= 1\n",
    "        new_size = cache_size_by_bins(curr_bins)\n",
    "        differences[i] = new_size - curr_size\n",
    "    return differences\n",
    "\n",
    "def infidelity_change_by_dir(tr_M_single, tr_M, indexes, n, bins, negative=False):\n",
    "    # check by how much the infidelity changes for every direction it could change in\n",
    "    curr_I = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=bins)\n",
    "    differences = np.zeros(len(bins))\n",
    "    for i in range(n):\n",
    "        curr_bins = bins.copy()\n",
    "        if not negative:\n",
    "            curr_bins[i] += 1\n",
    "        else:\n",
    "            curr_bins[i] -= 1\n",
    "        new_I = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=curr_bins)\n",
    "        differences[i] = curr_I - new_I \n",
    "    return differences\n",
    "\n",
    "def _find_solution(tr_M_single, tr_M, indexes, n, bins, I_tar=10**-14):\n",
    "    # increase to find solution\n",
    "    curr_I = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=bins)\n",
    "    while curr_I > I_tar:\n",
    "        inf_diff = infidelity_change_by_dir(tr_M_single, tr_M, indexes, n, bins)\n",
    "        cache_differences = cache_size_change_by_dir(bins)\n",
    "        curr_prices_of_optimization = inf_diff / cache_differences # infidelity decrease per cache increase\n",
    "        # any of the new points smaller than the target?\n",
    "        new_Is = curr_I - inf_diff\n",
    "        if np.any(new_Is < I_tar):\n",
    "            # find the one with the smallest increase in cache size\n",
    "            inds = np.where(new_Is < I_tar)\n",
    "            min_index = np.argmin(cache_differences[inds])\n",
    "            min_dir = inds[0][min_index]\n",
    "            bins[min_dir] += 1\n",
    "            curr_I = new_Is[min_dir]\n",
    "        else:\n",
    "            # find the one with the smallest cache price increase for the largest infidelity decrease\n",
    "            max_ind = np.argmax(curr_prices_of_optimization)\n",
    "            bins[max_ind] += 1\n",
    "            curr_I = new_Is[max_ind]\n",
    "    return bins, curr_I\n",
    "def _optimize_solution(tr_M_single, tr_M, indexes, n, bins, I_tar=10**-14):\n",
    "    # decrease for better solution\n",
    "    optimizing = True # while the current binnning is changing keep optimizing= True\n",
    "    curr_I = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=bins)\n",
    "    curr_cache = cache_size_by_bins(bins)\n",
    "    if curr_I > I_tar:\n",
    "        raise Exception('Initial solution not < I_tar')\n",
    "    new_Is = np.zeros(n)\n",
    "    new_caches = np.zeros(n, dtype=np.int64)\n",
    "    while optimizing:\n",
    "        # check neighbors I\n",
    "        for i in range(n):\n",
    "            curr_bins = bins.copy()\n",
    "            curr_bins[i] -= 1\n",
    "            if curr_bins[i] < 1:\n",
    "                new_Is[i] = 1.0\n",
    "            else:\n",
    "                new_Is[i] = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=curr_bins)\n",
    "            # cache smaller?\n",
    "            new_caches[i] = cache_size_by_bins(curr_bins)\n",
    "        # find I < I_tar among now_Is\n",
    "        inds = np.where(new_Is < I_tar)[0]\n",
    "        if len(inds) > 0:\n",
    "            min_index = np.argmin(new_caches[inds])\n",
    "            potential_ind = inds[min_index]\n",
    "            if new_caches[potential_ind] < curr_cache:\n",
    "                bins[potential_ind] -= 1\n",
    "                curr_I = new_Is[potential_ind]\n",
    "                curr_cache = new_caches[potential_ind]\n",
    "            else:\n",
    "                optimizing = False\n",
    "        else:\n",
    "            optimizing = False\n",
    "    return bins, curr_I\n",
    "\n",
    "def optimize_infidelity_old(tr_M_single, tr_M, indexes, n, bins=None, I_tar=10**-14):\n",
    "    # optimize binning to get minimum cache for target infidelity\n",
    "    # while I > I_tar: add bins in a direction that increases infidelity the most per cache size, if more than one direction would achieve the target fidelity \n",
    "    if bins is None:\n",
    "        bins = np.ones(n, dtype=np.int64)\n",
    "    bins, curr_I = _find_solution(tr_M_single, tr_M, indexes, n, bins, I_tar)\n",
    "    # check surrounding points for smaller cache size\n",
    "    bins, curr_I = _optimize_solution(tr_M_single, tr_M, indexes, n, bins, I_tar)\n",
    "    return bins, curr_I\n",
    "\n",
    "#### new optimization routine #################################################################################################\n",
    "# for every index i, separate the indexes and tr_M which contain i and those that dont\n",
    "def separate_indexes(i, indexes, tr_M):\n",
    "    # get indexes that contain i\n",
    "    inds_i = np.where(indexes[:,0] == i)[0]\n",
    "    inds_i = np.concatenate((inds_i, np.where(indexes[:,1] == i)[0]))\n",
    "    # get indexes that dont contain i, the inds not in inds_i\n",
    "    inds_not_i = np.setdiff1d(np.arange(len(indexes)), inds_i)\n",
    "    indexes_not_i = indexes[inds_not_i]\n",
    "    ind_i = indexes[inds_i]\n",
    "    # reduce indexes_i to not contain i, make it a one d array\n",
    "    indexes_i = np.empty(len(ind_i), dtype=np.int64)\n",
    "    for j in range(len(ind_i)):\n",
    "        indexes_i[j] = ind_i[j][0] if ind_i[j][0] != i else ind_i[j][1]\n",
    "    tr_M_not_i = tr_M[inds_not_i]\n",
    "    tr_M_i = tr_M[inds_i]\n",
    "    return indexes_i, indexes_not_i, tr_M_i, tr_M_not_i\n",
    "\n",
    "def separate_all_indexes(indexes, tr_M, n):\t\n",
    "    indexes_i = []\n",
    "    indexes_not_i = []\n",
    "    tr_M_i = []\n",
    "    tr_M_not_i = []\n",
    "    for i in range(n):\n",
    "        inds_i, inds_not_i, tr_M_i_, tr_M_not_i_ = separate_indexes(i, indexes, tr_M)\n",
    "        indexes_i.append(inds_i)\n",
    "        indexes_not_i.append(inds_not_i)\n",
    "        tr_M_i.append(tr_M_i_)\n",
    "        tr_M_not_i.append(tr_M_not_i_)\n",
    "    return indexes_i, indexes_not_i, tr_M_i, tr_M_not_i\n",
    "\n",
    "def I_rest_from_tr_M_bins(i, tr_M_single, tr_M_not_i, indexes_not_i, n, bins):\n",
    "    if bins is None:\n",
    "        bins = np.ones(n, dtype=np.int64)\n",
    "    param = 1 / bins\n",
    "    res = 0.0\n",
    "    if len(indexes_not_i) > 0:\n",
    "        multipliers_2 = param[indexes_not_i[:,0]]**2*param[indexes_not_i[:,1]]**2\n",
    "        res += np.sum(multipliers_2 * tr_M_not_i)\n",
    "    multipliers_4 = param**4\n",
    "    res +=  np.sum(multipliers_4[:i] * tr_M_single[:i]) + np.sum(multipliers_4[i+1:] * tr_M_single[i+1:])\n",
    "    return res\n",
    "\n",
    "def I_i_from_tr_M_bins(i, tr_M_single, tr_M_i, indexes_i, n, bins):\n",
    "    if bins is None:\n",
    "        bins = np.ones(n, dtype=np.int64)\n",
    "    param = 1 / bins\n",
    "    res = 0.0\n",
    "    if len(indexes) > 0:\n",
    "        multipliers_2 = param[indexes_i]**2\n",
    "        res += np.sum(multipliers_2 * tr_M_i) * param[i]**2\n",
    "    res +=  param[i]**4 * tr_M_single[i]\n",
    "    return res\n",
    "\n",
    "# optimize solution by reducing one bin direction by one and then finding the optimum along the other directions \n",
    "def optimum_along_single_direction(i, tr_M_single, tr_M_i, tr_M_not_i, indexes_i, indexes_not_i, n, bins, I_tar=10**-14):\n",
    "    # rewrite as qudratic expression, to get optimum along one direction\n",
    "    new_bins = bins.copy()\n",
    "    success = False\n",
    "    s = 1/bins**2\n",
    "    Pii =tr_M_single[i]\n",
    "    I = I_rest_from_tr_M_bins(i, tr_M_single, tr_M_not_i, indexes_not_i, n, bins) - I_tar\n",
    "    if I < 0:\n",
    "        Ci = np.sum(tr_M_i * s[indexes_i])\n",
    "        # find optimum along s_i -> quadratic expression in s_i\n",
    "        Ci_Pii = Ci/Pii/2\n",
    "        sqrt_internal = Ci_Pii**2 - I/Pii\n",
    "        if sqrt_internal > 0:\n",
    "            sqrt_term = np.sqrt(sqrt_internal)\n",
    "            s_i = - Ci_Pii - sqrt_term\n",
    "            if s_i < 0:\n",
    "                s_i = - Ci_Pii + sqrt_term\n",
    "            # find the \n",
    "            success = True if s_i > 0 else False\n",
    "            if success:\n",
    "                # from s_i to_bins[i]\n",
    "                bins_i = np.sqrt(1/s_i)\n",
    "                new_bins = bins.copy()\n",
    "                new_bins[i] = np.ceil(bins_i).astype(np.int64)\n",
    "    return new_bins, success\n",
    "\n",
    "def optimize_infidelity(tr_M_single, tr_M, indexes, n, bins=None, I_tar=10**-14):\n",
    "    # optimize binning to get minimum cache for target infidelity\n",
    "    # while I > I_tar: add bins in a direction that increases infidelity the most per cache size, if more than one direction would achieve the target fidelity \n",
    "    if bins is None:\n",
    "        bins = np.ones(n, dtype=np.int64)\n",
    "    bins, curr_I = _find_solution(tr_M_single, tr_M, indexes, n, bins, I_tar)\n",
    "    # check surrounding points for smaller cache size\n",
    "    bins, curr_I = _optimize_solution(tr_M_single, tr_M, indexes, n, bins, I_tar)\n",
    "\n",
    "    # separate indexes and tr_M\n",
    "    if len(indexes) > 0:\n",
    "        indexes_i, indexes_not_i, tr_M_i, tr_M_not_i = separate_all_indexes(indexes, tr_M, n)\n",
    "        # for every index i, reduce by one, check if solution is still in target range,\n",
    "        # if not, find which directions first inner point has lower cache size. if non has lower cache size, stop and go back to previous solution\n",
    "        last_bins = bins.copy()\n",
    "        found_lower = True\n",
    "        curr_min_cache = cache_size_by_bins(bins)\n",
    "        while found_lower:\n",
    "            found_lower = False\n",
    "            for i in range(n):\n",
    "                bin_red = last_bins.copy()\n",
    "                if bin_red[i] > 1:\n",
    "                    bin_red[i] -= 1\n",
    "                    #I_bin_red = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bin_red)\n",
    "                    #cache_bin_red = cache_size_from_bins(bin_red)\n",
    "                    # check for every other direction, which point leads to I < I_tar, and save the cache needed\n",
    "                    for j in range(n):\n",
    "                        if j == i:\n",
    "                            continue\n",
    "                        else:\n",
    "                            bin_j, success = optimum_along_single_direction(j, tr_M_single, tr_M_i[j], tr_M_not_i[j], indexes_i[j], indexes_not_i[j], n, bin_red, I_tar)\n",
    "                            if success:\n",
    "                                I_bin_j = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=bin_j)\n",
    "                                if not I_bin_j < I_tar:\n",
    "                                    raise ValueError('I_bin_j='+str(I_bin_j)+' should be smaller than I_tar, algorithm is broken. (' + str(bin_red) + '->' + str(bin_j) + ')')\n",
    "                                cache_bin_j = cache_size_by_bins(bin_j)\n",
    "                                if cache_bin_j < curr_min_cache:\n",
    "                                    curr_min_cache = cache_bin_j\n",
    "                                    bins = bin_j\n",
    "                                    found_lower = True\n",
    "            if found_lower:\n",
    "                last_bins = bins.copy()\n",
    "        bins = last_bins\n",
    "    return bins, curr_I\n",
    "\n",
    "#optimal_bins, I_exp = optimize_infidelity_new(tr_M_single, tr_M, indexes, n, I_tar=I_tar)\n",
    "#print(optimal_bins, I_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Number of Control Parameters:  1\n",
      "--------------------------------------------------\n",
      "Optimal Binning via Optimization:\n",
      "Bins:  [11] total: 11\n",
      "Est. I:  7.3744595224245e-13\n",
      "Actual I:  7.377431998634165e-13\n",
      "--------------------------------------------------\n",
      "Optimal Binning via New Optimization:\n",
      "Bins:  [11] total: 11\n",
      "Est. I:  7.3744595224245e-13\n",
      "Actual I:  7.377431998634165e-13\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "Optimal Binning via exhaustive Search:\n",
      "Bins:  [11] total: 11\n",
      "Est. I:  7.3744595224245e-13\n",
      "Actual I:  7.377431998634165e-13\n",
      "--------------------------------------------------\n",
      " \n",
      "--------------------------------------------------\n",
      "Number of Control Parameters:  2\n",
      "--------------------------------------------------\n",
      "Optimal Binning via Optimization:\n",
      "Bins:  [17  7] total: 119\n",
      "Est. I:  9.536336443187545e-13\n",
      "Actual I:  9.670042544485113e-13\n",
      "--------------------------------------------------\n",
      "Optimal Binning via New Optimization:\n",
      "Bins:  [17  7] total: 119\n",
      "Est. I:  9.536336443187545e-13\n",
      "Actual I:  9.670042544485113e-13\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "Optimal Binning via exhaustive Search:\n",
      "Bins:  [17  7] total: 119\n",
      "Est. I:  9.536336443187545e-13\n",
      "Actual I:  9.670042544485113e-13\n",
      "--------------------------------------------------\n",
      " \n",
      "--------------------------------------------------\n",
      "Number of Control Parameters:  3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Binning via Optimization:\n",
      "Bins:  [22  8  7] total: 1232\n",
      "Est. I:  9.59940856963182e-13\n",
      "Actual I:  1.1913803277252555e-12\n",
      "--------------------------------------------------\n",
      "Optimal Binning via New Optimization:\n",
      "Bins:  [22  8  7] total: 1232\n",
      "Est. I:  9.59940856963182e-13\n",
      "Actual I:  1.1913803277252555e-12\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "Optimal Binning via exhaustive Search:\n",
      "Bins:  [22  8  7] total: 1232\n",
      "Est. I:  9.59940856963182e-13\n",
      "Actual I:  1.1913803277252555e-12\n",
      "--------------------------------------------------\n",
      " \n",
      "--------------------------------------------------\n",
      "Number of Control Parameters:  4\n",
      "--------------------------------------------------\n",
      "Optimal Binning via Optimization:\n",
      "Bins:  [27 10  7  9] total: 17010\n",
      "Est. I:  9.784056991349676e-13\n",
      "Actual I:  1.0501599589929356e-12\n",
      "--------------------------------------------------\n",
      "Optimal Binning via New Optimization:\n",
      "Bins:  [27 10  7  9] total: 17010\n",
      "Est. I:  9.784056991349676e-13\n",
      "Actual I:  1.0501599589929356e-12\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "Optimal Binning via exhaustive Search:\n",
      "Bins:  [26  9  8  9] total: 16848\n",
      "Est. I:  9.964634203792034e-13\n",
      "Actual I:  1.0902390101819037e-12\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import Analysis_Code.discrete_quantum as dq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# Check the infidelity landscape as a function of binning\n",
    "for n in [1,2,3,4]:\n",
    "    print( ' ' )\n",
    "    print('-'*50)\n",
    "    print('Number of Control Parameters: ', n)\n",
    "    print('-'*50)\n",
    "    s = 16  # Hilbert space dimension\n",
    "    max_bins = 50\n",
    "    max_bins2 = 15\n",
    "    max_bins_list = [max_bins] + [max_bins2]*(n-1)\n",
    "    max_bins_range_list = [range(n) for n in max_bins_list]\n",
    "    rng = np.random.default_rng(100)\n",
    "    I_tar = 10**-12\n",
    "\n",
    "    rel_accs = np.empty(reps)\n",
    "    H_s = dq.Random_parametric_Hamiltonian_gauss(n, s, sigmas=np.pi/2*np.array([1.0,0.025, 0.01]), rng=rng)\n",
    "    tr_M_single, tr_M, tr_M_2, indexes, U_exact, U_approx = get_bidirectional_overlap_operators(H_s)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "\n",
    "    # now via optimization\n",
    "    print('Optimal Binning via Optimization:')\n",
    "    optimal_bins, I_exp = optimize_infidelity_old(tr_M_single, tr_M, indexes, n, I_tar=I_tar)\n",
    "    print('Bins: ',optimal_bins, ('total: '+str(np.prod(optimal_bins))))\n",
    "    print('Est. I: ', I_exp)\n",
    "    _,_,_,_, U_exact, U_approx = get_bidirectional_overlap_operators(H_s, bins=optimal_bins)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "    print('Actual I: ', dq.Av_Infidelity(U_exact, U_approx))\n",
    "    print('-'*50)\n",
    "\n",
    "    # now via new optimization\n",
    "    print('Optimal Binning via New Optimization:')\n",
    "    optimal_bins, I_exp = optimize_infidelity(tr_M_single, tr_M, indexes, n, I_tar=I_tar)\n",
    "    print('Bins: ',optimal_bins, ('total: '+str(np.prod(optimal_bins))))\n",
    "    print('Est. I: ', I_exp)\n",
    "    _,_,_,_, U_exact, U_approx = get_bidirectional_overlap_operators(H_s, bins=optimal_bins)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "    print('Actual I: ', dq.Av_Infidelity(U_exact, U_approx))\n",
    "    print('-'*50)\n",
    "\n",
    "    I_ = np.empty(max_bins_list, dtype=np.float64)\n",
    "    costs = -np.ones(max_bins_list, dtype=np.float64)\n",
    "    # Create iterator of all combinations of bins\n",
    "    ijk = itertools.product(*max_bins_range_list)\n",
    "    # iterate over all combinations of bins\n",
    "    for ind in ijk: #tqdm(ijk, total=max_bins*max_bins2**(n-1)):\n",
    "        bins = np.array(ind)+1\n",
    "        curr_I = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=bins)\n",
    "        I_[ind] = curr_I\n",
    "        if curr_I < I_tar:\n",
    "            costs[ind] = cache_size_by_bins(bins)\n",
    "\n",
    "    print('='*50)\n",
    "    # where is the minimum cost?\n",
    "    min_cost_indexes = np.where(costs == np.min(costs[costs > 0]))\n",
    "    optimal_bins = np.array([min_[0] for min_ in min_cost_indexes]).flatten() +1\n",
    "\n",
    "    print('Optimal Binning via exhaustive Search:')\n",
    "    print('Bins: ',optimal_bins, ('total: '+str(np.prod(optimal_bins))))\n",
    "    print('Est. I: ', I_[min_cost_indexes][0])\n",
    "    _,_,_,_, U_exact, U_approx = get_bidirectional_overlap_operators(H_s, bins=optimal_bins)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "    print('Actual I: ', dq.Av_Infidelity(U_exact, U_approx))\n",
    "    print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Results!\n"
     ]
    }
   ],
   "source": [
    "## Compare how accurate the optimization is\n",
    "# Check the infidelity landscape as a function of binning\n",
    "n_param = np.array([1,2,3,4])\n",
    "amps_lists = np.array([[1.0,0.025, 0.01], [1.0,0.025]], dtype=object)\n",
    "search_lists = np.array([[32,10], [30,30]])\n",
    "search_lists2 = np.array([[64,13], [40,40]])\n",
    "reps = 100\n",
    "\n",
    "filename = 'binning_optimization_results_n_'+str(n_param[0])+'_'+str(n_param[-1])+'.npz'\n",
    "dir = os.path.join(os.getcwd(), 'Cache')\n",
    "full_path = os.path.join(dir, filename)\n",
    "redo = True\n",
    "if os.path.exists(full_path):\n",
    "    redo = False\n",
    "    results = np.load(full_path, allow_pickle=True)\n",
    "    # check if input parameters have changed (n_param, amps_lists, search_lists, search_lists2, reps)\n",
    "    if not np.all(results['n_param'] == n_param):\n",
    "        redo = True\n",
    "    if not np.all(results['amps_lists'] == amps_lists):\n",
    "        redo = True\n",
    "    if not np.all(results['search_lists'] == search_lists):\n",
    "        redo = True\n",
    "    if not np.all(results['search_lists2'] == search_lists2):\n",
    "        redo = True\n",
    "    if not results['reps'] == reps:\n",
    "        redo = True\n",
    "    if redo == False:\n",
    "        print('Loaded Results!')\n",
    "        # unpack results\n",
    "        exhaust_I_est = results['exhaust_I_est']\n",
    "        exhaust_I_act = results['exhaust_I_act']\n",
    "        exhaust_cache_size = results['exhaust_cache_size']\n",
    "        optim_I_est = results['optim_I_est']\n",
    "        optim_I_act = results['optim_I_act']\n",
    "        optim_cache_size = results['optim_cache_size']\n",
    "    else:\n",
    "        print('Redoing Calculation!')\n",
    "else:\n",
    "    print('No Results Found!')\n",
    "if redo:\n",
    "    mat_size = (len(n_param), len(amps_lists), reps)\n",
    "    # exhaustive search\n",
    "    exhaust_I_est = np.empty(mat_size)\n",
    "    exhaust_I_act = np.empty(mat_size)\n",
    "    exhaust_cache_size = np.empty(mat_size, dtype=np.int64)\n",
    "    # optimisation\n",
    "    optim_I_est = np.empty(mat_size)\n",
    "    optim_I_act = np.empty(mat_size)\n",
    "    optim_cache_size = np.empty(mat_size, dtype=np.int64)\n",
    "    rng = np.random.default_rng(100)\n",
    "    for i, n in enumerate(n_param):\n",
    "        print('-'*50)\n",
    "        print('Number of Control Parameters: ', n)\n",
    "        for j, (a, sl, sl2) in enumerate(zip(amps_lists, search_lists, search_lists2)):\n",
    "            amps = np.pi/2*np.array(a)\n",
    "            print('Amplitudes: ', amps)\n",
    "            s = 16  # Hilbert space dimension \n",
    "            I_tar = 10**-12\n",
    "            for rep in tqdm(range(reps)):\n",
    "                rel_accs = np.empty(reps)\n",
    "                H_s = dq.Random_parametric_Hamiltonian_gauss(n, s, sigmas=amps, rng=rng)\n",
    "                tr_M_single, tr_M, tr_M_2, indexes, U_exact, U_approx = get_bidirectional_overlap_operators(H_s)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "\n",
    "                # now via optimization\n",
    "                optimal_bins, I_exp = optimize_infidelity(tr_M_single, tr_M, indexes, n, I_tar=I_tar)\n",
    "                cache_size = np.prod(optimal_bins)\n",
    "                _,_,_,_, U_exact, U_approx = get_bidirectional_overlap_operators(H_s, bins=optimal_bins)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "                I_actual = dq.Av_Infidelity(U_exact, U_approx)\n",
    "                optim_I_est[i,j,rep] = I_exp\n",
    "                optim_I_act[i,j,rep] = I_actual\n",
    "                optim_cache_size[i,j,rep] = cache_size\n",
    "\n",
    "                max_bins = sl[0] #64\n",
    "                max_bins2 = sl[1] #20\n",
    "                max_bins_list = [max_bins] + [max_bins2]*(n-1)\n",
    "                max_bins_range_list = [range(n) for n in max_bins_list]\n",
    "                I_ = np.empty(max_bins_list, dtype=np.float64)\n",
    "                costs = -np.ones(max_bins_list, dtype=np.float64)\n",
    "                # Create iterator of all combinations of bins\n",
    "                ijk = itertools.product(*max_bins_range_list)\n",
    "                # iterate over all combinations of bins\n",
    "                for ind in ijk: #tqdm(ijk, total=max_bins*max_bins2**(n-1)):\n",
    "                    bins = np.array(ind)+1\n",
    "                    curr_I = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=bins)\n",
    "                    I_[ind] = curr_I\n",
    "                    if curr_I < I_tar:\n",
    "                        costs[ind] = cache_size_by_bins(bins)\n",
    "\n",
    "                # where is the minimum cost?\n",
    "                if np.any(costs > 0):\n",
    "                    min_cost_indexes = np.where(costs == np.min(costs[costs > 0]))\n",
    "                else:\n",
    "                    max_bins = sl2[0] #64\n",
    "                    max_bins2 = sl2[1] #20\n",
    "                    max_bins_list = [max_bins] + [max_bins2]*(n-1)\n",
    "                    max_bins_range_list = [range(n) for n in max_bins_list]\n",
    "                    I_ = np.empty(max_bins_list, dtype=np.float64)\n",
    "                    costs = -np.ones(max_bins_list, dtype=np.float64)\n",
    "                    # Create iterator of all combinations of bins\n",
    "                    ijk = itertools.product(*max_bins_range_list)\n",
    "                    # iterate over all combinations of bins\n",
    "                    for ind in ijk: #tqdm(ijk, total=max_bins*max_bins2**(n-1)):\n",
    "                        bins = np.array(ind)+1\n",
    "                        curr_I = I_from_tr_M_bins(tr_M_single, tr_M, indexes, n, bins=bins)\n",
    "                        I_[ind] = curr_I\n",
    "                        if curr_I < I_tar:\n",
    "                            costs[ind] = cache_size_by_bins(bins)\n",
    "                    min_cost_indexes = np.where(costs == np.min(costs[costs > 0]))\n",
    "                    # combine min_cost indexes into one array\n",
    "                    #min_cost_indexes = np.array([np.array(ind) for ind in min_cost_indexes]).flatten()\n",
    "                    print('Larger search required...', min_cost_indexes)\n",
    "\n",
    "                optimal_bins = np.array([min_[0] for min_ in min_cost_indexes]).flatten() +1\n",
    "\n",
    "                cache_size = np.prod(optimal_bins)\n",
    "                I_est = I_[min_cost_indexes][0]\n",
    "                _,_,_,_, U_exact, U_approx = get_bidirectional_overlap_operators(H_s, bins=optimal_bins)  #tr_M_2 ## currently a sparse approach to infidelity calculation (only using the diagonal terms)\n",
    "                I_act = dq.Av_Infidelity(U_exact, U_approx)\n",
    "                exhaust_I_est[i,j,rep] = I_est\n",
    "                exhaust_I_act[i,j,rep] = I_act\n",
    "                exhaust_cache_size[i,j,rep] = cache_size\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    np.savez(full_path, n_param=n_param, amps_lists=amps_lists, search_lists=search_lists, search_lists2=search_lists2, reps=reps, exhaust_I_est=exhaust_I_est, exhaust_I_act=exhaust_I_act, exhaust_cache_size=exhaust_cache_size, optim_I_est=optim_I_est, optim_I_act=optim_I_act, optim_cache_size=optim_cache_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{c c c c c}\\hline\n",
      "n & Control Amplitudes & Cache Size & Rel. Acc. Cache Optimum & Rel. Acc. Infidelity \\\\\n",
      "\\hline\n",
      "$1$ & $[2.5,\\, 1]\\num{e-2}$ & $9.7 \\pm 1.10$ & $0 \\pm 0$ & $(1.6 \\pm 1.2)\\num{e-3}$ \\\\\n",
      "$1$ & $[2.5]\\num{e-2}$ & $9.6 \\pm 1.7$ & $0 \\pm 0$ & $(1.7 \\pm 1.3)\\num{e-3}$ \\\\\n",
      "$2$ & $[2.5,\\, 1]\\num{e-2}$ & $(1 \\pm 0.3)e2$ & $(1.9 \\pm 6.8)\\num{e-3}$ & $(8.7 \\pm 8)\\num{e-2}$ \\\\\n",
      "$2$ & $[2.5,\\, 2.5]\\num{e-2}$ & $(2.4 \\pm 0.7)e2$ & $(9.3 \\pm 44.1)\\num{e-4}$ & $(2.10 \\pm 2.6)\\num{e-2}$ \\\\\n",
      "$3$ & $[2.5,\\, 1,\\, 1]\\num{e-2}$ & $(1.3 \\pm 0.5)e3$ & $(4.6 \\pm 12.3)\\num{e-3}$ & $(8.9 \\pm 6.5)\\num{e-2}$ \\\\\n",
      "$3$ & $[2.5,\\, 2.5,\\, 2.5]\\num{e-2}$ & $(7.8 \\pm 2.6)e3$ & $(1.1 \\pm 3.7)\\num{e-3}$ & $(7.4 \\pm 5.4)\\num{e-2}$ \\\\\n",
      "$4$ & $[2.5,\\, 1,\\, 1,\\, 1]\\num{e-2}$ & $(2 \\pm 0.9)e4$ & $(2.6 \\pm 4.3)\\num{e-2}$ & $(8.10 \\pm 7.6)\\num{e-2}$ \\\\\n",
      "$4$ & $[2.5,\\, 2.5,\\, 2.5,\\, 2.5]\\num{e-2}$ & $(3.1 \\pm 1.3)e5$ & $(5.8 \\pm 12.4)\\num{e-3}$ & $(9.4 \\pm 6.4)\\num{e-2}$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# analyze how well the optimization did\n",
    "# calculate the accuracy of the infidelity estimation\n",
    "# for every value calculate mean and std\n",
    "rel_I_est2act = np.abs((optim_I_act - optim_I_est)/optim_I_act)\n",
    "rel_I_est_mean = np.mean(rel_I_est2act, axis=-1)\n",
    "rel_I_est_std = np.std(rel_I_est2act, axis=-1)\n",
    "\n",
    "rel_cache_est2act = np.abs((optim_cache_size - exhaust_cache_size)/exhaust_cache_size)\n",
    "rel_cache_est_mean = np.mean(rel_cache_est2act, axis=-1)\n",
    "rel_cache_est_std = np.std(rel_cache_est2act, axis=-1)\n",
    "mean_cache = np.mean(optim_cache_size, axis=-1)\n",
    "std_cache = np.std(optim_cache_size, axis=-1)\n",
    "\n",
    "# Make table of results for latex\n",
    "names = ['n', 'Control Amplitudes', 'Cache Size', 'Rel. Acc. Cache Optimum', 'Rel. Acc. Infidelity']\n",
    "str_funs = ['i', 'le1', 'pm', 'p2', 'p2']\n",
    "# create variables for table\n",
    "n_par, amps, cache, rel_cache, rel_I = [], [], [], [], []\n",
    "for i, n in enumerate(n_param):\n",
    "    for j, a in enumerate(amps_lists):\n",
    "        n_par.append(n)\n",
    "        # reformat a\n",
    "        a2 = [a_ for a_ in a[1:]]\n",
    "        while len(a2) < n:\n",
    "            a2.append(a2[-1])\n",
    "        amps.append(a2)\n",
    "        cache.append([mean_cache[i,j], std_cache[i,j]])\n",
    "        rel_cache.append([rel_cache_est_mean[i,j], rel_cache_est_std[i,j]])\n",
    "        rel_I.append([rel_I_est_mean[i,j], rel_I_est_std[i,j]])\n",
    "arrays = [n_par, amps, cache, rel_cache, rel_I]\n",
    "latex_str = latex_table(names, str_funs, arrays, vert_lines=False)\n",
    "print(latex_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
